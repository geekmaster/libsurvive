Based on Oliver Kreylos' claims that he measured vive accuracy at about 1/3 mm, and Alan Yates' claims that the sync pulse illumination only reliably reaches 3m (the 16ft recommended LH placement), if we use a LH to illuminate a 120x120 degree patch inside a 3m radius dome, I calculated that a LH is the equivalent of a 200 megapixel camera, in comparison to how Oculus does camera-based tracking. And this dome projection idea is what keeps occuring to me in my dreams (a 64K pixel circumference sphere onto which I map LH radial coordinates using 16-bit integer axis values). I tend to dream in integer algorithms because of a lifetime of computer programming where floats were done in software (and I am fascinated with DDAs, having independently invented Bresenham's algorithm before my first contact with the real thing, back in the early 70s). Sadly, most dreams fade quickly unless they build upon what I work on during the days when fully awake. Though I get momentary flashbacks during showering and shaving, if my dreams are important to my current project.(edited)
When I map that 200 megapixel LH view to fill a sphere, it gives me a 2 gigapixel equirectangular spherical projection (64K by 32K). Most equirectangular video and images are only 4K by 2K pixels, so when viewing a tracked object (or a LH) in VR in the vive, we are tracking to about 1/16th of a pixel coordinate (H or V). A single pixel in a 4K image represents 16x16 (64K) LH tracked positions.

I am just trying to document my dreams here (which happened again last night). When they keep repeating, I remember more of them. They are now trying to escape into reality, requiring me to document and implement them. This poser thing is intuitive for me, not based on existing solutions that I may be aware of. Not sure how they will work out IRL...
Questions or comments are welcome, but this is what I can remember at this point. I suspect more details will percolate out when I least expect it (showering, shaving, etc.).
One nice thing about using a 200 megapixel equirectangular canvas in VR is that it not only matches vive positional accuracy (as reported), but it allows zooming in up to 16x in the vive HMD without loss of detail. Though that is a pretty big framebuffer -- not that much of a problem even in an odroid-c2 (considering that it has 10x that much RAM). Even using a full world backbuffer would only consume 20% of RAM.

And BTW, equirectangular projections are already much like MIP maps, especially near the top and bottom, so using a framebuffer horizontal stride that varies sinusoidally along the vertical axis should suffice (i.e. nearest-neighbor in the smeared regions). But at 16x magnification to fit LH resolutions, we probably need to average 16 pixels at each stride in horizontal direction. Dividing by 16 is conveniently just a shift, so a low-cost operation  for downsampling to fit the vive screens.(edited)

I just looked. Kreylos says 0.3mm accuracy (limited by noise) expands to 2.1mm in the distance (from LH) direction when using only one LH. But even at 0.3mm resolution, a single vive HMD onscreen pixel shift corresponds to about 5mm movement of the WM controller in your hand (or much more when moving toward or away from a single LH when they other LH is obstructed). Kreylos says "This anisotropy is to be expected, as Lighthouse-based pose estimation boils down to the same perspective-n-point reconstruction problem thatâ€™s encountered in camera-based tracking and has lateral-to-camera error that grows linearly with distance, and distance-to-camera error that grows quadratically with distance." His description of his noise-measurement setup places the sensors about 2.8m  from the LH (assuming 400mm chair height), so it also fits the 3m dome distance that I have been envisioning in my dreams.(edited)
*** Am I sharing too many "stream of consciousness" thoughts here? I am afraid of losing them with HDD corruption (and forgetting my dreams), so SOMEPLACE online seems a good choice. I want feedback, so HERE seems like a better choice than elsewhere.(edited)
I am thinking of projected pose pattern recognition on my dome surface, with 15-degree pose rotations on each axis (like the hours on a 24-hr clock). I have a competing thought of 30 spherical directions as facebook uses in their pyramidal spherical video encoding, each with 24 rotations on the distance axis. Those decisions are competing in my dreams, so I need to explore them in my thoughts (which I am streaming here).
